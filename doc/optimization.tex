\chapter{Lattice Correction and Design}
\label{c:opti}

\index{Optimization|hyperbf}
\index{Merit function}
This chapter covers the process of \vn{optimization} which involves
minimization of a
\vn{Merit Function} to correct and to design lattices. Examples
of \vn{lattice corrections} include flattening the orbit and adjusting
quadrupoles to correct the measured betatron phase. \vn{Lattice
design} involves creating a lattice that conforms to a set of
desirable properties. For example, requiring that the beta function
in a certain region never exceeds a given value.
Section~\sref{s:optimizer} gives a tutorial on how to setup the merit
function and run the \vn{optimizer} in \tao.  In this chapter,
Section~\sref{s:lattice.correction} presents the merit function in the
context of lattice corrections while Section~\sref{s:lattice.design}
discuses the merit function in the context of lattice design. Since
the concepts used in \vn{lattice corrections} and \vn{lattice design} are
similar, \tao combines the two into one generalized process as 
discussed in Section~\sref{s:generalized.design}.

%------------------------------------------------------------------------
\section{Lattice Corrections}
\label{s:lattice.correction}
\index{Modeling Data}
\index{Lattice Corrections}

\index{Optimization!Merit Function}
Consider the problem of problem of modifying the orbit of a beam
through a lattice to conform to some desired orbit (typically a ``flat'' orbit
running through the centers of the quadrupoles). The process
generally goes through three stages: First the orbit is measured, then
corrections to the steering elements are calculated and finally the
corrections are applied to the machine. Since these are necessarily
machine specific, \tao has no specific routines to measure orbits or
to load steering corrections but they could be implemented as
discussed in Chapter~\sref{c:prog.customizing}. \tao does, however,
implement a generalized algorithm procedure for minimizing a
\vn{merit function} which can be used to calculate the corrections.
The idea is to vary a set of variables (steerings in the case of an
orbit correction) within the \vn{model} lattice
(\sref{s:universe}) with the aim to make the measured \vn{data}
(position data for an orbit correction) correspond to the
values as calculated from the \vn{model} lattice.

The merit function \vn{M} that is a measure of how
well \vn{data_model}, the data as calculated from the \vn{model}, fits
\vn{data_meas}, the measured data, is
\Begineq
  {\cal M} \equiv \sum_{i} w_i \,
    \bigl[ \data\_\model(i) -  \data\_\meas(i) \bigr]^2 + 
  \sum_{j} w_j \,
    \bigl[ \var\_\model(j) - \var\_\meas(j) \bigr]^2
  \label{m1}
\Endeq
\vn{var_model} is the value of a variable in the \vn{model} and
\vn{var_meas} is the value as measured at the time the data was taken
(for example, by measuring the current through a steering and using a
calibration factor to calculate the kick) and the sum \vn{j} runs over
all variables that are allowed to be varied to minimize \vn{M}.  The
second term in the merit function prevents degeneracies (or near
degeneracies) in the problem which would allow \tao to find solutions
where \vn{data_model} matches \vn{data_measured} with the
\vn{var_model} having ``unphysical'' values (values far from
\vn{var_meas}). The weights $w_i$ and $w_j$ need to be set depending
upon how accurate the measured data is relative to how accurate the
calibrations for measuring the \vn{var_meas} values are. With the
second term in the merit function, the number of constraints (number of
terms in the merit function) is always larger than the number of
variables and degeneracies can never occur.

In a correction one wants to change the machine variables so that the
measured data corresponds to the design values \vn{data_design}. Thus
the change in the data that one wants is
\begin{example}
  data_change = data_design - data_meas
\end{example}
Once a fit has been made, and presuming that the \vn{data_model} is
reasonably close to the \vn{data_meas} this data change within the
\vn{model} lattice can be accomplished by changing the variables by
\begin{example}
  var_change = var_design - var_model
\end{example}
This assumes the system is linear. For many situations this is true
since typically \vn{var_change} is ``small''. Since the variables have
a measured value of \vn{var_meas} the value that the variables should
be set to is
\begin{example}
  var_final = var_meas + (var_design - var_model)
\end{example}
Notice that the fitting process is independent of the \vn{design}
lattice. It is only when calculating the corrections to the
variables that the \vn{design} lattice plays a role. 

Sometimes it is desired to fit to changes in data as opposed to the
absolute value of the data. For example, when closing an orbit bump
knob what is important is the difference in orbits before and after
the bump knob is varied. Designating one of these orbit the
\vn{reference}, the appropriate merit function is
\begin{alignat}{1}
  {\cal M} = &\sum_{i} w_i \,
    \Bigl[ \bigl( \data\_\model(i) - \data\_\design(i) \bigr) - 
      \bigl( \data\_\meas(i) - \data\_\reference(i) \bigr) \Bigr]^2 + \CRNO
  &\sum_{j} w_j \,
    \Bigl[ \bigl( \var\_\model(j) - \var\_\design(j) \bigr) -
     \bigl( \var\_\meas(i) - \var\_\reference(i) \bigr) \Bigr]^2 
  \label{m2}
\end{alignat}
where \vn{data_ref} and \vn{var_ref} refer to the reference
measurement.  This merit function is acceptable if the reference data
is taken with the machine reasonably near the design setup so that
nonlinearities can be ignored. If this is not the case then the
fitting becomes a two step process: The first step is to fit the
\vn{model} to the \vn{reference} data using the merit function of
\Eq{m1}. The \vn{base} lattice is then set equal to the \vn{model}
lattice. The second step is to fit the model using the merit function
\begin{alignat}{1}
  {\cal M} = &\sum_{i} w_i 
    \Bigl[ \bigl( \data\_\model(i) - \data\_\base(i) \bigr) - 
      \bigl( \data\_\meas(i) - \data\_\reference(i) \bigr) \Bigr]^2 + \CRNO
  &\sum_{j} w_j 
    \Bigl[ \bigl( \var\_\model(j) - \var\_\base(j) \bigr) -
     \bigl( \var\_\meas(i) - \var\_\reference(i) \bigr) \Bigr]^2 
  \label{m3}
\end{alignat}

Control of what data and what variables are to be used in the fitting
process is controlled by the \vn{use}, \vn{veto}, \vn{restore}, and
\vn{clip} commands.

%------------------------------------------------------------------------
\section{Lattice Design}
\label{s:lattice.design}
\index{Optimization!Lattice Design}
\index{Optimization!Constraints}

Lattice design is the process of calculating \vn{variable} strengths
to meet a number of criteria called constraints. For example, one
constraint could be that the beta function in some part of the lattice
not exceed a certain value. In this case we can proceed as was done
for lattice corrections and define a merit function to be minimized:
\Begineq
  {\cal M} = \sum_{i} w_i \, C_i^2
\Endeq
Typically constraints are either to limit values to some range so the
constraint would be of the form
\Begineq
  C = 
    \begin{cases}
    \mbox{model} - \mbox{limit}  & \mbox{model $>$ Limit} \\
    0                            & \mbox{otherwise}
    \end{cases}
\Endeq
or a constraint is used to keep the \vn{model} at a certain value so
the form of the constraint would be
\Begineq
  C = \mbox{model} - \mbox{target}  
\Endeq
Here \vn{model} is the value as calculated from the \vn{model}
lattice. \vn{target} and \vn{limit} are given numbers. Part of the
optimization process is in deciding what the values should be for any
\vn{target} or \vn{limit}.

%------------------------------------------------------------------------
\section{Generalized Design}
\label{s:generalized.design}
\index{Optimization!Generalized Merit function}

Since lattice correction and design are similar, \tao
combines the two into one generalized process. In this
generalized process, the merit function becomes
\Begineq
  {\cal M} = \sum_i w_i \, D_i^2 + \sum_j w_j \, V_j^2
  \label{miwdjw}
\Endeq
where $D_i$ are the data terms and $V_j$ are the variable terms.
The general form of the data merit terms $D_i$ is 
\Begineq
  D = 
    \begin{cases}
    \mbox{delta}  & \mbox{Non-Zero-Condition} \\
    0             & \mbox{Otherwise}
    \end{cases}
\Endeq
The equation used to calculate \vn{delta} is determined by two global logicals called
\vn{opt_with_ref} and \vn{opt_with_base} (\sref{s:globals}) as shown in
Table~\ref{t:delta}. 
\begin{table}[ht] 
\centering 
{\tt
\begin{tabular}{|l|l|l|} \hline
  \vn{Opt_with_ref} & \vn{Opt_with_base} & \vn{delta} \\ \hline 
  F & F & Model - Meas                \\ \hline 
  T & F & Model - Meas + Ref - Design \\ \hline 
  F & T & Model - Meas - Base         \\ \hline 
  T & T & Model - Meas + Ref - Base   \\ \hline 
\end{tabular}
} 
\caption{The form of \vn{delta}}  
\label{t:delta}
\end{table}
An exception occurs when using a \vn{common base lattice}
(\sref{s:cbl}). In this case, the common universe does not have base
or reference values associated with it. Thus all data and variables
that are associated with the common universe calculate their
\vn{delta} as if both \vn{opt_with_ref} and \vn{opt_with_base} were
set to \vn{False}.

Another exception occurs with data when the datum value cannot be
computed (\sref{s:datum.opt}). In this case, the datum's \vn{invalid} value
is used for the \vn{delta}. This is useful, for example, in a linear
lattice when the particle trajectory results in the particle being lost.

The \vn{Non-Zero-Condition} needed for a non--zero $D_i$ is dependent
upon the \vn{merit_type} of the datum (\sref{s:data.anatomy}).
There are five \vn{merit_type} constraint
types as given in Table~\ref{t:con.type}.
\begin{table}[ht]
\centering
{\tt
\begin{tabular}{|l|l|l|} \hline
  {\it Merit\_Type}       & {\it Non-zero-Condition} \\ \hline 
  \vn{target}            & Any \vn{delta}   \\ \hline 
  \vn{min}, \vn{abs_min} & \vn{delta} $<$ 0 \\ \hline 
  \vn{max}, \vn{abs_max} & \vn{delta} $>$ 0 \\ \hline 
\end{tabular}
}
\caption{Constraint Type List.}
\label{t:con.type}
\end{table}
\index{Optimization!Optimize with Reference}

For variables, the form of the terms $V_i$ is determined by its \vn{merit_type}.
Here the \vn{merit_type} may be:
\begin{example}
  target
  limit
\end{example}
A \vn{target} \vn{merit_type} for a variable is the same as for
datum. In this case \vn{model} is just the value of the variable.
A \vn{limit} \vn{merit_type} has the form
\Begineq
  V = 
    \begin{cases}
    \mbox{model} - \mbox{high\_lim}  & \mbox{model} > \mbox{high\_lim} \\
    \mbox{model} - \mbox{low\_lim}   & \mbox{model} < \mbox{low\_lim} \\
    0                               & \mbox{Otherwise}
    \end{cases}
\Endeq
The default \vn{merit_type} for a variable is \vn{limit}.

Note: when doing lattice design \vn{opt_with_ref} and
\vn{opt_with_base} are both set to \vn{False} and the \vn{target} and
\vn{limit} values are identified with \vn{Meas}.

When optimizing a storage ring, If the ring is unstable so that the
twiss parameters, closed orbit, etc. cannot be computed, the
contribution to the merit function from the corresponding datums is
set to zero. This tends to lower the merit function and in this case
an optimizer will never leave the unstable region. To avoid this,
an \vn{unstable_ring} constraint (\sref{s:data.types}) must be set.

To see a list of constraints when running \tao use the \vn{show
constraints} command (\sref{s:show}). To see how a particular variable
or datum is doing use the \vn{show data} or \vn{show variable}
commands.  See \sref{s:datum.opt} for details on how datums are
chosen to be included in an optimization.

%------------------------------------------------------------------------
\section{Optimizers in Tao}
\label{s:tao.opti}
\index{Optimization!Optimizer}

The algorithm used to vary the \vn{model} variables to minimize \vn{M}
is called an \vn{optimizer}. In \vn{command line mode} the \vn{run}
command is used to invoke an \vn{optimizer}. In \vn{single mode} the
\vn{g} key starts an optimizer. In both modes the period key
(\vn{``.''}) stops the optimization.  Running an optimizer is also
called ``fitting'' since one is trying to get the \vn{model} data to
be equal to the \vn{measured} data. With orbits this is also called
``flattening'' since one generally wants to end up with an orbit that
is on--axis.

The optimizer that is used can be defined when using the \vn{run} command but
the default optimizer can be set in the \tao input file by setting the
\vn{global%optimizer} component (\sref{s:globals}).

When the optimizer is run in \tao, the optimizer, after it initializes
itself, takes a number of \vn{cycles}. Each cycle consists of changing
the values of the variables the optimizer is allowed to change. The
number of steps that the optimizer will take is determined by the
parameter \vn{global%n_opti_cycles} (\sref{s:globals}). When the
optimizer initializes itself and goes through
\vn{global%n_opti_cycles}, it is said to have gone through one
\vn{loop}. After going through through \vn{global%n_opti_loops} loops,
the optimizer will automatically stop.  To immediately stop the
optimizer the period key \vn{``.''} may be pressed. Note: In
\vn{single_mode} (\sref{c:single}), \vn{n_opti_loops} is ignored and
the optimizer will loop forever.

There are currently three optimizers that can be used: 
  \begin{description}
  \index{lm optimizer}
  \item{\vn{lm}} \Newline
\vn{lm} is an optimizer based upon the Levenburg-Marquardt algorithm
as implemented in \vn{Numerical Recipes}\cite{b:nr}. This algorithm
looks at the local derivative matrix of \vn{dData/dVariable} and takes
steps in variable space accordingly. The derivative matrix is
calculated beforehand by varying all the variables by an amount set by
the variable's \vn{step} component (\sref{s:init.var}). The \vn{step}
size should be chosen large enough so that round-off errors will not
make computation of the derivatives inaccurate but the step size
should not be so large that the derivatives are effected by
nonlinearities. By default, the derivative matrix will be recalculated
each \vn{loop} but this can be changed by setting the
\vn{global%derivative_recalc} global parameter (\sref{s:globals}). The
reason to not recalculate the derivative matrix is one of time.
However, if the calculated derivative matrix is not accurate (that is,
if the variables have changed enough from the last time the matrix was
calculated and the nonlinearities in the lattice are large enough),
the \vn{lm} optimizer will not work very well.  In any case, this
method will only find local minimum.
  \index{lmdif optimizer}
  \item{\vn{lmdif}} \Newline
The \vn{lmdif} optimizer is like the \vn{lm} optimizer except that it
builds up the information it needs on the derivative matrix by
initially taking small steps over the first \vn{n} cycles where \vn{n}
is the number of variables. The advantage of this is that you do not
have to set a \vn{step} size for the variables. The disadvantage is
that for \vn{lmdif} to be useful, the number of \vn{cycles} must be
greater than the number of variables. Again, like \vn{lm}, this method
will only find local minimum.
  \index{de optimizer}
  \item{\vn{de}} \Newline
The \vn{de} optimizer stands for \vn{differential
evolution}\cite{b:de}. The advantage of this optimizer is that it
looks for global minimum. The disadvantage is that it is slow to find
the bottom of a local minimum. A good strategy sometimes when trying
to find a global minimum is to use \vn{de} in combination with \vn{lm}
or \vn{lmdif} one after the other. One important parameter with the
\vn{de} optimizer is the \vn{step} size. A larger step size means that
the optimizer will tend to explore larger areas of variable space but
the trade off is that this will make it harder to find minimum in the
locally. One good strategy is to vary the \vn{step} size to see what
is effective. Remember, the optimal step size will be different for
different problems and for different starting points. The \vn{step}
size that is appropriate of the \vn{de} optimizer will, in general, be
different from the \vn{step} size for the \vn{lm} optimizer. For this
reason, and to facilitate changing the step size, the actual step size
used by the \vn{de} optimizer is the step size given by a variable's
\vn{step} component multiplied by the global variable
\vn{global%de_lm_step_ratio}. This global variable can be varied using
the \vn{set} command (\sref{s:set}). The number of trial solutions used 
in the optimization is
\begin{example}
  population = number_of_variables * global%de_var_to_population_factor
\end{example}
There are also a number of parameters that can be set that will affect
how the optimizer works. See Section~\sref{s:globals} for more
details.
  \index{svd optimizer}
  \item{\vn{svd}} \Newline
The \vn{svd} optimizer uses a sigular value decomposition calculation.
See the description of \vn{svdfit} from Numerical Recipes\cite{b:nr}
for more details. With the \vn{svd} optimizer, the setting of 
\vn{global%n_opti_cycles}is ignored. 
  \end{description}

%------------------------------------------------------------------------
\section{Common Base Lattice (CBL) Analysis}
\label{s:cbl}
\index{Common Base Lattice}

Some data analysis problems involve varying variables in a both the
\vn{model} and \vn{base} lattices simultaneously. Such is the case
with Orbit Response Matrix (\vn{ORM}) analysis\cite{b:orm}. With
\vn{ORM}, the analysis starts with a set of difference orbits. A given
difference orbit is generated by varying a given steering by a known
amount and the steering varied is different for different difference
orbits. Typically, The number $N$ of difference orbits is equal to the
number of steering elements in the machine. In \tao, this will result
in the creation of $N$ universes, one for each difference
measurement. The \vn{model} lattice in a universe will correspond to
the machine with the corresponding steering set to what it was when
the data was taken. Conversely, the \vn{base} lattices in all the
universes all correspond to the common condition without any steering
variation.

In \tao, this arrangement is called \vn{Common Base Lattice}
(\vn{CBL}) analysis. To do a CBL analysis, the \vn{common_lattice}
switch must be set at initialization time (\sref{s:init.lat}).  With
\vn{CBL}, \tao will set up a \vn{``common''} universe with index 0.
The \vn{model} lattice of this common universe will be used as the
\vn{base} lattice for all universes. 

The variables (fit parameters) in a \vn{CBL} analysis can be divided
into two classes. One class consists of the parameters that were
varied to get the data of the different universes. With \vn{ORM},
these are the steering strengths. At initialization
(\sref{s:init.var}), variables must be set up that control these
parameters. A single variable will control that particular parameter in
a particular universe, that was varied to create the data for that
universe. 

The second class of variables consists of everything that is
to be varied in the common base lattice. With \vn{ORM}, this generally
will include such things as quadrupole and BPM error tilts, etc. That
is, parameters that did {\em not} change during data taking. The
\tao variables that are created for these parameters will control
parameters of the \vn{model} lattice in the common universe.

To cut down on memory usage when using \vn{CBL} (the number of data
sets, hence the number of universes, can be very large), \tao does
not, except for the common \vn{model} lattice, reserve separate memory
for each \vn{model} lattice. Rather, it reserves memory for a single
\vn{``working''} lattice and the \vn{model} lattice for a particular
universe is created by first copying the common \vn{base} lattice to
the \vn{working} lattice and then applying the variable(s) (a steering
in the case of \vn{ORM}) appropriate for that universe.  As a result,
except for the common \vn{model} lattice, it is not possible to vary a
parameter of a \vn{model} lattice unless that parameter has a \tao
variable that associated with it. The \vn{change} command
(\sref{s:change}) is thus restricted to always vary parameters in
the common \vn{model} lattice.

With \vn{CBL}, the \vn{opt_with_base} and \vn{opt_with_ref}
(\sref{s:generalized.design}) global logicals are generally set to
True. Since \vn{opt_with_base}, and \vn{opt_with_ref} do not make
sense when applied to the data in the common universe, The
contribution to the merit function from data in this universe is
always calculated as if \vn{opt_with_base} and \vn{opt_with_ref} were
set to False.

With \vn{opt_with_base} set to True, the \vn{base} value for a
datum is evaluated by looking for a corresponding datum in the common
universe and using its \vn{model} value. To simplify the bookkeeping,
it is assumed that the structure of the data arrays is identical from
universe to universe. That is, the \vn{show data} command gives
identical results independent of the viewed universe. 
